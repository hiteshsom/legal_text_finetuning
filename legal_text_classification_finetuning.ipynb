{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be50e050-0c2f-4c41-9479-493aa90c4a08",
   "metadata": {},
   "source": [
    "# Legal Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2acd8-a4fb-484c-beaf-4c6f446f2fc4",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81bb754f-5c9b-4ab9-8591-b75b0e914ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.33.1\n",
    "# !pip install torch==2.1.0\n",
    "# !pip install accelerate -U\n",
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56628135-d571-4b95-b48b-1e97d367924b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hitesh somani\\documents\\hobby_projects\\text_classification\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch\n",
    "import transformers\n",
    "import random\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208f9aad-aac5-4523-adb3-28e71504fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"legal_text_classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bbe1598-2c25-41fa-9051-a4b88ed04ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24985, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c58182-3c5a-43fa-90d0-e0cff4949eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24985 entries, 0 to 24984\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   case_id       24985 non-null  object\n",
      " 1   case_outcome  24985 non-null  object\n",
      " 2   case_title    24985 non-null  object\n",
      " 3   case_text     24809 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 780.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3be296f-e69a-4987-a962-d2f0f912cb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "case_id           0\n",
       "case_outcome      0\n",
       "case_title        0\n",
       "case_text       176\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d859f25e-ac67-4754-8d89-93418f70928e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>case_outcome</th>\n",
       "      <th>case_title</th>\n",
       "      <th>case_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Case29</td>\n",
       "      <td>followed</td>\n",
       "      <td>Elderslie Finance Corp Ltd v Australian Securi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id case_outcome                                         case_title  \\\n",
       "24  Case29     followed  Elderslie Finance Corp Ltd v Australian Securi...   \n",
       "\n",
       "   case_text  \n",
       "24       NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['case_text'].isna(), :].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3159625d-027c-4fc6-abb5-741703d40e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>case_outcome</th>\n",
       "      <th>case_title</th>\n",
       "      <th>case_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case1</td>\n",
       "      <td>cited</td>\n",
       "      <td>Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...</td>\n",
       "      <td>Ordinarily that discretion will be exercised s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case2</td>\n",
       "      <td>cited</td>\n",
       "      <td>Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...</td>\n",
       "      <td>The general principles governing the exercise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case3</td>\n",
       "      <td>cited</td>\n",
       "      <td>Colgate Palmolive Co v Cussons Pty Ltd (1993) ...</td>\n",
       "      <td>Ordinarily that discretion will be exercised s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case4</td>\n",
       "      <td>cited</td>\n",
       "      <td>Dais Studio Pty Ltd v Bullett Creative Pty Ltd...</td>\n",
       "      <td>The general principles governing the exercise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case5</td>\n",
       "      <td>cited</td>\n",
       "      <td>Dr Martens Australia Pty Ltd v Figgins Holding...</td>\n",
       "      <td>The preceding general principles inform the ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  case_id case_outcome                                         case_title  \\\n",
       "0   Case1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
       "1   Case2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
       "2   Case3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
       "3   Case4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
       "4   Case5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
       "\n",
       "                                           case_text  \n",
       "0  Ordinarily that discretion will be exercised s...  \n",
       "1  The general principles governing the exercise ...  \n",
       "2  Ordinarily that discretion will be exercised s...  \n",
       "3  The general principles governing the exercise ...  \n",
       "4  The preceding general principles inform the ex...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "413f9a7c-7afd-4b0b-87aa-551d54f938db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The general principles governing the exercise of the discretion to award indemnity costs after rejection by an unsuccessful party of a so called Calderbank letter were set out in the judgment of the Full Court in Black v Lipovac [1998] FCA 699 ; (1998) 217 ALR 386. In summary those principles are: 1. Mere refusal of a \"Calderbank offer\" does not itself warrant an order for indemnity costs. In this connection it may be noted that Jessup J in Dais Studio Pty Ltd v Bullet Creative Pty Ltd [2008] FCA 42 said that (at [6]): if the rejection of such an offer is to ground a claim for indemnity costs, it must be by reason of some circumstance other than that the offer happened to comply with the Calderbank principle. 2. To obtain an order for indemnity costs the offeror must show that the refusal to accept it was unreasonable. 3. The reasonableness of the conduct of the offeree is to be viewed in the light of the circumstances that existed when the offer was rejected.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1, 'case_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d9f14-5e68-4692-8770-5e1053db6ff6",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c2231a-56da-45cb-9c5e-8f38edd9d3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting to class labels: 100%|███████████████████████████████| 24985/24985 [00:00<00:00, 154938.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert the pandas DataFrame to a Hugging Face dataset\n",
    "df = df.rename(columns={'case_outcome': 'label'})\n",
    "data = datasets.Dataset.from_pandas(df)\n",
    "data = data.class_encode_column(\"label\")\n",
    "\n",
    "# Perform a stratified train-test split test set 90%, some of the classes are very less so better to stratify\n",
    "data = data.train_test_split(test_size=0.1, stratify_by_column='label', seed=10)\n",
    "\n",
    "\n",
    "num_classes = data['train'].features['label'].num_classes\n",
    "id2label = {i:data['train'].features['label'].int2str(i) for i in range(num_classes)}\n",
    "label2id = {label:i for (i,label) in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8932ba87-a3ee-41fd-a739-b3da623b2442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['case_id', 'label', 'case_title', 'case_text'],\n",
       "        num_rows: 22486\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['case_id', 'label', 'case_title', 'case_text'],\n",
       "        num_rows: 2499\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7f1453d-c227-477f-97d1-798263f055f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'affirmed',\n",
       " 1: 'applied',\n",
       " 2: 'approved',\n",
       " 3: 'cited',\n",
       " 4: 'considered',\n",
       " 5: 'discussed',\n",
       " 6: 'distinguished',\n",
       " 7: 'followed',\n",
       " 8: 'referred to',\n",
       " 9: 'related'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d0c2fae-cd74-447e-a9ce-008c5218c4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['affirmed', 'applied', 'approved', 'cited', 'considered', 'discussed', 'distinguished', 'followed', 'referred to', 'related'], id=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'].features['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c64d51e-f477-4713-80fd-2519c2c14945",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8440802-9c45-4d49-ba3c-28944402fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case outcome is label column, case_title and case_text we can merge in one column as they both might contain\n",
    "# some important textual information. For example: if one compant names in case_title judgement is most of the time any one class label\n",
    "# in such a case including case_title is important. case_text has a lot of information about case which might influence label column\n",
    "\n",
    "def merge_title_text(example):\n",
    "    example['text'] = \"Case Title: \" + example['case_title'] + str(\"\" if example['case_text'] is None else \"\\nCase Text: \" + example['case_text'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6812616d-cdc3-4d88-b6d0-53852241dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████| 22486/22486 [00:03<00:00, 7343.51 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████| 2499/2499 [00:00<00:00, 7228.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.map(merge_title_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5760dfbb-5452-48d6-b3f3-530503e4cacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['case_id', 'label', 'case_title', 'case_text', 'text'],\n",
       "        num_rows: 22486\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['case_id', 'label', 'case_title', 'case_text', 'text'],\n",
       "        num_rows: 2499\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8c30514-c5c6-42b4-ad86-d2d36494c4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Title: Comandate Marine Corporation v The Ship \"Boomerang I\" [2006] FCAFC 106 ; (2006) 151 FCR 403\n",
      "Case Text: course, there is an incongruity in this approach because it ignores the rights of a secured creditor (other than a holder of a maritime lien recognised in s 15) such as a mortgagee and instead prefers those of a co-owner. Thus, if a vessel is co-owned it would not be able to be arrested under s 19 if one co-owner were not a relevant person under s 19(a), but a mortgagee cannot escape the amenability of the vessel to arrest. But this is the consequence of the legislative choice of selecting, as the criterion for actuating the right defined in s 19(b), the \"owner\", and not extending this to secured creditors or demise charterers: cf Comandate Marine Corporation v The Ship \"Boomerang I\" [2006] FCAFC 106 ; (2006) 151 FCR 403. As Allsop J observed, the wide group of categories identified in s 19(a) is then \"limited to the more narrow funnel in para (b) ...\": \" Boomerang I \" 151 FCR at 411 [35]. Notions of ownership \"are not amenable to crisp, comprehensive definition in the abstract\": Tisand Pty Ltd v The Owners of the Ship MV Cape Moreton (Ex Freya) [2005] FCAFC 68 ; (2005) 143 FCR 43 at 73 [119] per Ryan and Allsop JJ; see too \" Boomerang I \" 151 FCR at 411 [33] per Allsop J. Ryan and Allsop JJ said that in the context of ss 17, 18 and 19 of the Act the words \"ownership\" or \"owner\" connote the right to have and dispose of dominion, possession and enjoyment of the ship, subject to intervening interests. The word \"owner\" is used in those sections in a proprietary sense. This reflects the purpose of the Act to require any property arrested to belong to the relevant person.\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2691e08f-7295-443f-9e47-5679339d9932",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "664a2d08-8f1c-4fb7-9422-3c36467aa55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "898f66b6-627b-4e0e-a039-a7b2bc9b400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate input text to be not more than distibert maximum imput limit\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cf6a333-01c8-4535-bb95-287a68d35f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████| 22486/22486 [00:16<00:00, 1328.87 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████| 2499/2499 [00:01<00:00, 1299.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f3b9d9b-4128-4ac7-b5ca-f67b7f7be4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['case_id', 'label', 'case_title', 'case_text', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 22486\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['case_id', 'label', 'case_title', 'case_text', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2499\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74ce00d1-2142-40cd-866b-9dfc1a6e9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# For padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bc8c9d8-8fca-40d9-9f6c-bd162ac03e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "532db8e7-f2f2-4e3e-b9d1-9990646ca4e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0.004536155830294405},\n",
       " {1: 0.0979720715111625},\n",
       " {2: 0.004313795250378013},\n",
       " {3: 0.4890598594681135},\n",
       " {4: 0.06853153073023215},\n",
       " {5: 0.04100329093658276},\n",
       " {6: 0.024326247442853333},\n",
       " {7: 0.09027839544605533},\n",
       " {8: 0.17544249755403363},\n",
       " {9: 0.004536155830294405}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_outcome = list(set(data['train']['label']))\n",
    "\n",
    "counts = [0]*len(set_outcome)\n",
    "\n",
    "list(map(lambda x, y: {y: (x+data['train']['label'].count(y))/len(data['train']['label'])}, counts, set_outcome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88cf5b4c-07b6-4a87-be83-0e98f3eec244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: 0.004401760704281713},\n",
       " {1: 0.09803921568627451},\n",
       " {2: 0.004401760704281713},\n",
       " {3: 0.4889955982392957},\n",
       " {4: 0.06842737094837935},\n",
       " {5: 0.04081632653061224},\n",
       " {6: 0.024409763905562223},\n",
       " {7: 0.09043617446978791},\n",
       " {8: 0.1756702681072429},\n",
       " {9: 0.004401760704281713}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x, y: {y: (x+data['test']['label'].count(y))/len(data['test']['label'])}, counts, set_outcome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7b321f-e138-4c59-9f73-f727e4de7eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "256af844-c462-4231-8c3a-80a4c31e4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f17452c-ae80-4122-9852-2208ecdcb6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We can see precision and recall later first lets try accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcd7fd6d-ef8c-4ffe-936e-6df48cd3264f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Distilbert becuase its small, easy to fit in memory, we can try Lora and Peft for more memory optimization later and we can also\n",
    "# try some models who have been trained on legal domain\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=10, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf9c9a62-8bd1-4b39-972f-9492c513b86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hitesh somani\\documents\\hobby_projects\\text_classification\\venv\\lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2811' max='2811' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2811/2811 11:01:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.482400</td>\n",
       "      <td>1.438468</td>\n",
       "      <td>0.492197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2811, training_loss=1.516077389712013, metrics={'train_runtime': 39730.428, 'train_samples_per_second': 0.566, 'train_steps_per_second': 0.071, 'total_flos': 2954456857070400.0, 'train_loss': 1.516077389712013, 'epoch': 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"finetuned_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data[\"train\"],\n",
    "    eval_dataset=data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c07eaa-dbe0-4367-93f0-69250b198f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87c5aad2-f7d1-4c9f-96cd-4959d5c77c36",
   "metadata": {},
   "source": [
    "### Saving model for backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "648bbfbf-8377-4b93-8d1a-d3f216f4a5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('finetuned_model_backup\\\\tokenizer_config.json',\n",
       " 'finetuned_model_backup\\\\special_tokens_map.json',\n",
       " 'finetuned_model_backup\\\\vocab.txt',\n",
       " 'finetuned_model_backup\\\\added_tokens.json',\n",
       " 'finetuned_model_backup\\\\tokenizer.json')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('finetuned_model_backup')\n",
    "tokenizer.save_pretrained('finetuned_model_backup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03aa4a5-a56a-4a7d-9e30-0c3a3d26c90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8d2f79c-ea1a-4ad8-af5c-03edf603d014",
   "metadata": {},
   "source": [
    "### Calculating other metrics on eval data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d285c45-cd6f-4978-b977-c8eefd042efa",
   "metadata": {},
   "source": [
    "It was necessary to check Precision, Recall and F1 since, all classes are not balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c20b13a-a40e-407d-a8e4-9f813a86ea17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small_data = data['test'].select(range(100))\n",
    "predictions = trainer.predict(data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81b96b88-214c-4f41-9987-cfef182a8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "75c62f3a-71e8-470e-893e-064870c8cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to compute your metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    precision = precision_score(labels, predictions, average='weighted')\n",
    "    recall = recall_score(labels, predictions, average='weighted')\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "88f35cd3-547c-4d19-a888-cf2929c3a1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.3058893677564232, 'recall': 0.4921968787515006, 'f1': 0.3468256056594664}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hitesh somani\\documents\\hobby_projects\\text_classification\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics((predictions.predictions, predictions.label_ids))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "130d242b-2596-4353-b317-87620498367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ca88e0fd-2db2-4a9a-ad26-fd59b8bcda0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hitesh somani\\documents\\hobby_projects\\text_classification\\venv\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "c:\\users\\hitesh somani\\documents\\hobby_projects\\text_classification\\venv\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "c:\\users\\hitesh somani\\documents\\hobby_projects\\text_classification\\venv\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3058893677564232\n",
      "Recall: 0.4921968787515006\n",
      "F1-score: 0.3468256056594664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hitesh somani\\documents\\hobby_projects\\text_classification\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Load the metrics\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "preds = torch.argmax(torch.from_numpy(predictions.predictions), dim=-1)\n",
    "preds = preds.numpy()\n",
    "label = predictions.label_ids\n",
    "\n",
    "# Compute the metrics\n",
    "precision = precision_metric.compute(predictions=preds, references=label, average=\"weighted\")[\"precision\"]\n",
    "recall = recall_metric.compute(predictions=preds, references=label, average=\"weighted\")[\"recall\"]\n",
    "f1 = f1_metric.compute(predictions=preds, references=label, average=\"weighted\")[\"f1\"]\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42586f6-a753-4878-8a1b-46a0d2021ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e00ac-9e81-43a5-b186-211393d7c7f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "legal_text_classification_venv",
   "language": "python",
   "name": "legal_text_classification_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
